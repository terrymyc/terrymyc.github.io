@article{maDailyWebSurvey,
  title = {Daily Web Survey Data Collection of Time-Varying Cannabis Use Motives and Contexts, with Implications for Adaptive Interventions: {{A}} Pilot Study},
  author = {Ma, Yongchao and West, Brady T. and McCabe, Sean Esteban},
  journaltitle = {Drug and Alcohol Dependence},
  pubstate = {submitted},
  keywords = {journal}
}

@article{maOptimalStratificationMethod2025,
  title = {An Optimal Stratification Method for Addressing Nonresponse Bias in {{Bayesian}} Adaptive Survey Design},
  author = {Ma, Yongchao and Mushkudiani, Nino and Schouten, Barry},
  date = {2025-06-02},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/00491241251345463},
  url = {https://journals.sagepub.com/doi/10.1177/00491241251345463},
  urldate = {2025-06-11},
  abstract = {In a probability sampling survey, adaptive data collection strategies may be used to obtain a response set that minimizes nonresponse bias within budget constraints. Previous research has stratified the target population into subgroups defined by categories of auxiliary variables observed for the entire population, and tailored strategies to obtain similar response rates across subgroups. However, if the auxiliary variables are weakly correlated with the target survey variables, optimizing data collection for these subgroups may not reduce nonresponse bias and may actually increase the variance of survey estimates. In this paper, we propose a stratification method to identify subgroups by: (1) predicting values of target survey variables from auxiliary variables, and (2) forming subgroups with different response propensities based on the predicted values of target survey variables. By tailoring different data collection strategies to these subgroups, we can obtain a response set with less variation in response propensities across subgroups that are directly relevant to the target survey variables. Given this rationale, we also propose to measure nonresponse bias by the coefficient of variation of response propensities estimated from the predicted target survey variables. A case study using the Dutch Health Survey shows that the proposed stratification method generally produces less variation in response propensities with respect to the predicted target survey variables compared to traditional methods, thereby leading to a response set that better resembles the population.},
  langid = {english},
  keywords = {journal}
}

@article{vandeschootOpenSourceMachine2021,
  title = {An Open Source Machine Learning Framework for Efficient and Transparent Systematic Reviews},
  author = {Van De Schoot, Rens and De Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and De Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
  date = {2021-02-01},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {3},
  number = {2},
  pages = {125--133},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00287-7},
  url = {https://www.nature.com/articles/s42256-020-00287-7},
  urldate = {2025-04-20},
  abstract = {Abstract             To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
  langid = {english},
  keywords = {journal},
  file = {/Users/yongchao/Zotero/storage/8UISHZKP/Van De Schoot et al. - 2021 - An open source machine learning framework for effi.pdf}
}

@article{westLatentTransitionAnalysis2024,
  title = {Latent Transition Analysis of Time-Varying Cannabis Use Motives to Inform Adaptive Interventions},
  author = {West, Brady T. and Ma, Yongchao and Lankenau, Stephen and Wong, Carolyn F. and Bonar, Erin E. and Patrick, Megan E. and Walton, Maureen A. and McCabe, Sean Esteban},
  date = {2024-11},
  journaltitle = {Psychology of Addictive Behaviors},
  shortjournal = {Psychology of Addictive Behaviors},
  volume = {38},
  number = {7},
  pages = {759--771},
  issn = {1939-1501, 0893-164X},
  doi = {10.1037/adb0001012},
  url = {https://doi.apa.org/doi/10.1037/adb0001012},
  urldate = {2025-04-20},
  langid = {english},
  keywords = {journal}
}
